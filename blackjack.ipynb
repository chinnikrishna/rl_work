{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackJack\n",
    "\n",
    "Using monte carlo methods to solve blackjack env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space is Discrete(2)\n",
      "Observation space is Tuple(Discrete(32), Discrete(11), Discrete(2))\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "print(\"Action space is \" + str(env.action_space))\n",
    "print(\"Observation space is \" + str(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In monte carlo methods we start with a random policy and interact with environment to populate a Q table. Q table has states on row and actions as columns. Then (s,a) corresponds to expected return if agents starts in that state and takes that action. Each occurence of (s,a) pair is called visit.\n",
    "\n",
    "Let us start with a random policy where player will stick hand with 80% prob (or hit with 20% prob) if sum is greater than 18 else keep asking for hits with 80% prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(env):\n",
    "    episode = []\n",
    "    # Reset the environment\n",
    "    state = env.reset()\n",
    "    player_hand, opp_hand, ace = state\n",
    "    \n",
    "    # Defining the policy\n",
    "    if player_hand > 18:\n",
    "        stick_prob = [0.8, 0.2]\n",
    "    else:\n",
    "        stick_prob = [0.2, 0.8]\n",
    "    \n",
    "    # Playing an episode with the set policy\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(2), p=stick_prob)\n",
    "        ns, reward, done, info = env.step(action)\n",
    "        # Record action, next state and reward\n",
    "        episode.append((ns, action, reward))\n",
    "        state = ns\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing first visit MC prediction, stick=0, hit=1\n",
    "\n",
    "def every_visit(env, num_episodes, gamma=0.9):\n",
    "    # Store the reward with next state as key and action as index to make a Q table\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum =  defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    for epi_idx in range(num_episodes):\n",
    "        # Generate an episode\n",
    "        episode = random_policy(env)\n",
    "        # Separate the episode into state, action and rewards\n",
    "        states, action, reward = zip(*episode)\n",
    "        # Generate gamma for discounting rewards\n",
    "        discounts = np.array([gamma**i for i in range(len(reward)+1)])\n",
    "        # Now iterate through states in episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][action[i]] += sum(reward[i:] * discounts[:-(1+i)])\n",
    "            N[state][action[i]] += 1\n",
    "            Q[state][action[i]] = returns_sum[state][action[i]] / N[state][action[i]]\n",
    "    return Q\n",
    "\n",
    "Q = every_visit(env, 10000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(env, Q):\n",
    "    # Use random policy if state is not present in Q table else use Q table\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    # Defining the random policy for fall back\n",
    "    player_hand, opp_hand, ace = state\n",
    "    if player_hand > 18:\n",
    "        stick_prob = [0.8, 0.2]\n",
    "    else:\n",
    "        stick_prob = [0.2, 0.8]\n",
    "    \n",
    "    while True:\n",
    "        if state in Q:\n",
    "            # Choose greedily\n",
    "            action = np.argmax(Q[state])\n",
    "        else:\n",
    "            action = np.random.choice(np.arange(2), p=stick_prob)\n",
    "        ns, reward, done, info = env.step(action)\n",
    "        # Record action, next state and reward\n",
    "        episode.append((ns, action, reward))\n",
    "        state = ns\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
